<p>[TOC]</p>
<h2>课程内容</h2>
<ol>
<li>爬虫简介</li>
<li>简单爬虫架构</li>
<li>URL管理器 ：管理待抓取URL集合和已抓取URL集合
    防止重复抓取、防止循环抓取</li>
<li>网页下载器(urllib2)</li>
<li>网页解析器(BeautifulSoup)</li>
<li>完整实例
    爬取百度百科Python词条相关的1000个页面数据</li>
</ol>
<h2>网页下载器-urllib2</h2>
<h3>方法1：最简洁方法</h3>
<pre><code class="language-python">import ur1lib2
#直接请求
response=urllib2.urlopen(http://www.baidu.com')
#获取状态码，如果是2ee表示获取成功
print response.getcode()
#读取内容
cont=response.read()
</code></pre>
<h3>方法2：添加data、http header import urllib2</h3>
<pre><code class="language-python">#创建Request对象
request=ur11ib2.Request(ur1)
#添加数据
request.add_data(a'，‘12)
#添加http的header request.add_header(‘User-Agent'，‘Mozilla/5.e')
#发送请求获取结果
response=ur1lib2.urlopen(request)
</code></pre>
<h3>方法3：添加特殊情景的处理器：HTTPCookieProcessor, ProxyHandler, HTTPSHandler, HTTPRedirectHandler</h3>
<pre><code class="language-python">import ur1lib2，cookielib
#创建cookie容器
cj=cookielib.CookieJar()
#创建1个opener opener=ur11ib2.build_opener(ur1lib2.HTTPCookieProcessor(cj)
#给ur11ib2安装opener ur1lib2.install_opener(opener)
#使用带有cookie的ur11ib2访问网页
response=url1ib2.urlopen(&quot;http://www.baidu.com/&quot;)
</code></pre>
<p>三种方法的示例：</p>
<pre><code class="language-python">import urllib2
ur1=&quot;http://www.baidu.com&quot;


print'第一种方法’
response1=ur11ib2.urlopen(ur1)
print responsel.getcode()
print len(response1.read())


print“第二种方法”
request=ur1lib2.Request(url)
request.add_header(&quot;user-agent&quot;，&quot;Mozilla/5.e&quot;)
response2=ur11ib2.urlopen(request)
print response2.getcode()
print len(response2.read())


print‘第三种方法’
cj=cookielib.CookieJar()
opener=ur11ib2.build_opener(ur11ib2.HTTPCookieProcessor(cj))
ur11ib2.install_opener(opener)
response3=ur11ib2.urlopen(ur1)
print response3.getcode()
print cj
print response3.read()
</code></pre>
<h2>网页解析器</h2>
<h3>Python有哪几种网页解析器？</h3>
<ul>
<li>正则表达式： 模糊匹配</li>
<li>html.parser： 结构化解析</li>
<li>Beautiful Soup： 结构化解析</li>
<li>lxml ： 结构化解析</li>
</ul>
<h2>Beautiful Soup</h2>
<p>-安装：pip install beautifulsoup4
-测试：import bs4</p>
<h3>解析步骤</h3>
<ul>
<li>创建 BeautifulSoup对象</li>
<li>搜索节点 find all、find<ul>
<li>按节点名称</li>
<li>按节点属性值</li>
<li>按节点文字</li>
</ul>
</li>
<li>访问节点： 名称、属性、文字</li>
</ul>
<pre><code class="language-python">from bs4 import BeautifulSoup
#根据HTML网页字符串创建Beautifulsoup对象
soup=BeautifulSoup(
    html_doc，#HTML文档字符串
    'html.parse',  #HTML解析器
    from_encoding=‘utf8' #HTML文档的编码
)


#方法：find_all(name，attrs，string)
#查找所有标签为a的节点
soup.find_all('a')
#查找所有标签为a，链接符合/view/123.htm形式的节点soup.find_all(a'，href=/view/123.htm')
soup.find_all('a'，href=re.compile(r&quot;/view/\d+\.html&quot;))
#查找所有标签为div，class为abc，文字为Python的节点
soup.find_all(div'，class_=‘abc'，string=‘Python')


#得到节点：&lt;a href=‘1.htm12&gt;Python&lt;/a&gt;
#获取查找到的节点的标签名称
node.name
#获取查找到的a节点的href属性
node[‘href2]
#获取查找到的a节点的链接文字
node.get_text()
</code></pre>
<h2>Beautiful Soup实例</h2>
<pre><code class="language-python">soup=BeautifulSoup ( html_doc，‘html.parser'，from_encoding='utf-8' )
print‘获取所有的链接”
1inks=soup.find_all ( 'a' )
for link in links：print link.name，link['href']，link.get_text ( )
print‘获取Lacie的链接’
link node =soup.find ( 'a'，href='http://example.com/Lacie' )
print link_node.name，link_node['href']，link_node.get_text ( )
print '正则匹配'
link_node=soup.find('a',href=re.compile(r&quot;ill&quot;))
print‘获P段落文字
pnode=soup.find ( 'p'，class_=&quot;title&quot; )
print p_node.name，p_nodel.get_text ( )
</code></pre>