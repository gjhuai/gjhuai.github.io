<p>[TOC]</p>
<h1>BeautifulSoup</h1>
<h2>课程内容</h2>
<ol>
<li>爬虫简介</li>
<li>简单爬虫架构</li>
<li>URL管理器 ：管理待抓取URL集合和已抓取URL集合
    防止重复抓取、防止循环抓取</li>
<li>网页下载器(urllib2)</li>
<li>网页解析器(BeautifulSoup)</li>
<li>完整实例
    爬取百度百科Python词条相关的1000个页面数据</li>
</ol>
<h2>网页下载器-urllib2</h2>
<h3>方法1：最简洁方法</h3>
<pre><code class="language-python">import ur1lib2
#直接请求
response=urllib2.urlopen(http://www.baidu.com')
#获取状态码，如果是2ee表示获取成功
print response.getcode()
#读取内容
cont=response.read()
</code></pre>
<h3>方法2：添加data、http header import urllib2</h3>
<pre><code class="language-python">#创建Request对象
request=ur11ib2.Request(ur1)
#添加数据
request.add_data(a'，‘12)
#添加http的header request.add_header(‘User-Agent'，‘Mozilla/5.e')
#发送请求获取结果
response=ur1lib2.urlopen(request)
</code></pre>
<h3>方法3：添加特殊情景的处理器：HTTPCookieProcessor, ProxyHandler, HTTPSHandler, HTTPRedirectHandler</h3>
<pre><code class="language-python">import ur1lib2，cookielib
#创建cookie容器
cj=cookielib.CookieJar()
#创建1个opener opener=ur11ib2.build_opener(ur1lib2.HTTPCookieProcessor(cj)
#给ur11ib2安装opener ur1lib2.install_opener(opener)
#使用带有cookie的ur11ib2访问网页
response=url1ib2.urlopen(&quot;http://www.baidu.com/&quot;)
</code></pre>
<p>三种方法的示例：</p>
<pre><code class="language-python">import urllib2
ur1=&quot;http://www.baidu.com&quot;


print'第一种方法’
response1=ur11ib2.urlopen(ur1)
print responsel.getcode()
print len(response1.read())


print“第二种方法”
request=ur1lib2.Request(url)
request.add_header(&quot;user-agent&quot;，&quot;Mozilla/5.e&quot;)
response2=ur11ib2.urlopen(request)
print response2.getcode()
print len(response2.read())


print‘第三种方法’
cj=cookielib.CookieJar()
opener=ur11ib2.build_opener(ur11ib2.HTTPCookieProcessor(cj))
ur11ib2.install_opener(opener)
response3=ur11ib2.urlopen(ur1)
print response3.getcode()
print cj
print response3.read()
</code></pre>
<h2>网页解析器</h2>
<h3>Python有哪几种网页解析器？</h3>
<ul>
<li>正则表达式： 模糊匹配</li>
<li>html.parser： 结构化解析</li>
<li>Beautiful Soup： 结构化解析</li>
<li>lxml ： 结构化解析</li>
</ul>
<h2>Beautiful Soup</h2>
<p>-安装：pip install beautifulsoup4
-测试：import bs4</p>
<h3>解析步骤</h3>
<ul>
<li>创建 BeautifulSoup对象</li>
<li>搜索节点 find all、find<ul>
<li>按节点名称</li>
<li>按节点属性值</li>
<li>按节点文字</li>
</ul>
</li>
<li>访问节点： 名称、属性、文字</li>
</ul>
<pre><code class="language-python">from bs4 import BeautifulSoup
#根据HTML网页字符串创建Beautifulsoup对象
soup=BeautifulSoup(
    html_doc，#HTML文档字符串
    'html.parse',  #HTML解析器
    from_encoding=‘utf8' #HTML文档的编码
)


#方法：find_all(name，attrs，string)
#查找所有标签为a的节点
soup.find_all('a')
#查找所有标签为a，链接符合/view/123.htm形式的节点soup.find_all(a'，href=/view/123.htm')
soup.find_all('a'，href=re.compile(r&quot;/view/\d+\.html&quot;))
#查找所有标签为div，class为abc，文字为Python的节点
soup.find_all(div'，class_=‘abc'，string=‘Python')


#得到节点：&lt;a href=‘1.htm12&gt;Python&lt;/a&gt;
#获取查找到的节点的标签名称
node.name
#获取查找到的a节点的href属性
node[‘href2]
#获取查找到的a节点的链接文字
node.get_text()
</code></pre>
<h2>Beautiful Soup实例</h2>
<pre><code class="language-python">soup=BeautifulSoup ( html_doc，‘html.parser'，from_encoding='utf-8' )
print‘获取所有的链接”
1inks=soup.find_all ( 'a' )
for link in links：print link.name，link['href']，link.get_text ( )
print‘获取Lacie的链接’
link node =soup.find ( 'a'，href='http://example.com/Lacie' )
print link_node.name，link_node['href']，link_node.get_text ( )
print '正则匹配'
link_node=soup.find('a',href=re.compile(r&quot;ill&quot;))
print‘获P段落文字
pnode=soup.find ( 'p'，class_=&quot;title&quot; )
print p_node.name，p_nodel.get_text ( )
</code></pre>
<h1>Scrapy爬虫框架</h1>
<p><img alt="" src="_v_images/2019-11-29-17-57-09.png" /></p>
<p>scrapy爬虫项目开发分成四步：新建项目、明确目标、制作爬虫、存储内容</p>
<h2>安装</h2>
<p>pip install scrapy
编译Twised时需要Visual C++ Build Tools，可以在这来下载安装：<a href="http://www.xdowns.com/soft/38/138/2017/soft_226169.html#download_box">Microsoft Visual C++ Build Tools</a></p>
<h2>新建scrapy项目</h2>
<ul>
<li>新建项目
<code>scrapy startproject douban</code></li>
<li>douban\spiders
进入douban\spiders目录，执行：<code>scrapy genspider douban_spider movie.douban.com</code></li>
</ul>
<h2>明确目标</h2>
<p>分析<code>https://movie.douban.com/top250</code>，需要抓取序号、电影名、导演、演员、星级、评价数、描述等。</p>
<pre><code class="language-python"># 序号
serial_number = scrapy.Field()
# 电影的名称
movie_name = scrapy.Field()
# 电影的介绍
introduce = scrapy.Field()
# 星级
star = scrapy.Field()
# 电影的评论数
evaluate = scrapy.Field()
# 电影的描述
describe = scrapy.Field()
</code></pre>
<h2>爬虫文件编写（制作爬虫）</h2>
<pre><code class="language-python">class DoubanSpiderSpider(scrapy.Spider):
    # 爬虫名称
    name = 'douban_spider'
    # 允许的域名
    allowed_domains = ['movie.douban.com']
    # 入口url，扔到调度器里面去
    start_urls = ['https://movie.douban.com/top250']


    def parse(self, response):
        print(response.text)
</code></pre>
<p>进入douban\spiders目录，执行：<code>scrapy crawl douban_spider</code>
windows下运行会出现错误：</p>
<blockquote>
<p>Could not find a version that satisfies the requirement win32api (from versions: )  No matching distribution found for win32api
解决办法：<code>pip install pypiwin32</code></p>
</blockquote>
<p>远程运行，新建main.py</p>
<pre><code class="language-python">from scrapy import cmdline
cmdline.execute('scrapy crawl douban_spider'.split())
</code></pre>
<h2>保存数据</h2>
<h2>常见问题处理</h2>
<p><a href="https://www.jianshu.com/p/830ca5623211">scrapy模拟登录代码演示及cookie原理说明</a></p>